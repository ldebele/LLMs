{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb8b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa650481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a875d933",
   "metadata": {},
   "source": [
    "### FLAN-t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inialize the tokenizer\n",
    "tokernizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# inialize the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74469e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"translate English to German: How old are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the text into embedded\n",
    "input_ids = tokernizer(prompt, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    temperature=0,\n",
    "    )\n",
    "\n",
    "# convert the generated output into text\n",
    "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1cf1f",
   "metadata": {},
   "source": [
    "### Phi 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29d845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "215604bb",
   "metadata": {},
   "source": [
    "### Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74599fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f72692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502939d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ae498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7930c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\", padding_side=left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e620d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCaualLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.3\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb290f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=250,\n",
    "    temperature=0,\n",
    "    )\n",
    "\n",
    "# convert the generated output into text\n",
    "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c29f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e273910",
   "metadata": {},
   "source": [
    "### Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-9b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698481f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer the input text\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27157e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the output\n",
    "outputs = model.generate(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the generated output into text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc59ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
